{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#install transformers from huggingface\n",
    "\n",
    "! git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD is now at 143738214 Fix the loss calculation of ProphetNet (#13132)\n"
     ]
    }
   ],
   "source": [
    "! cd transformers && git reset --hard 143738214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /home/student/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m681.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK\n",
    "\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from tokenizers) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.12.2)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "# Install Tokenizers\n",
    "\n",
    "! pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "# Install sentencepiece\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-01-31 20:59:36.942151: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-31 20:59:37.316871: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-31 20:59:37.319823: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-31 20:59:38.163899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# Initialize our model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"orzhan/t5-long-extract\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"orzhan/t5-long-extract\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filenameWithPath:str)->str:\n",
    "  '''\n",
    "  This function reads the content of a file.\n",
    "\n",
    "  Parameters:\n",
    "  filenameWithPath : str\n",
    "    Absolute Path of the file which is to be read.\n",
    "\n",
    "  Returns:\n",
    "  text : str\n",
    "    The content of the read file. \n",
    "\n",
    "  '''\n",
    "  file = open(filenameWithPath,\"r\")\n",
    "  text = file.read().strip()\n",
    "  text = re.sub(\"\\n\",\" \",text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "  '''\n",
    "  The preprocessing function which parses raw text and manages to clean the raw text. This module removes websites/URLs, Email IDs, redundant spaces, extra periods, dashes and commas. It also replace some special unicode apostrope, qoutation symbols with ' and  \". This module also ads extra space after period, question mark and exclamation mark.\n",
    "\n",
    "  Parameters:\n",
    "    text : str\n",
    "      The raw text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "    text : str\n",
    "      The preprocessed text. \n",
    "  '''\n",
    "  # text = text.lower()\n",
    "  text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" \", text)\n",
    "  text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', \" \",text)\n",
    "  text = re.sub(\"\\u2019\", '\\'', text)\n",
    "  text = re.sub(\"\\u2018\", '\\'', text)\n",
    "  text = re.sub(\"\\u201C\", '\\\"', text)\n",
    "  text = re.sub(\"\\u201D\", '\\\"', text)\n",
    "  # text = re.sub(\"[$]\", \"dollar \", text)\n",
    "  # text = re.sub(\"[£]\", \"pound \", text)\n",
    "  # text = re.sub(\"[%]\", \" percent\", text)\n",
    "  # text = re.sub(r\"[^a-zA-Z0-9?!.,’-]\", ' ', text)\n",
    "  text = re.sub(r\"([?!])\", r\" \\1 \", text)\n",
    "  text = re.sub(r',+', ',', text)\n",
    "  text = re.sub(r'[-]+', ' ', text)\n",
    "  text = re.sub(r'([a-z])(?=[.,])', r'\\1 ', text)\n",
    "  text = re.sub(r'\\.{2,}', '.', text)\n",
    "  text = re.sub(r\"\\s+\", \" \", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSentences(text):\n",
    "    '''\n",
    "    This function uses the nltk library to sentence tokenize a given corpus or document.\n",
    "\n",
    "    Parameters:\n",
    "    text : \n",
    "        The corpus or document string to split into sentences.\n",
    "\n",
    "    Returns: \n",
    "        A sentence tokenized copy of *text* using NLTK's default tokenizer. A list of sentences.\n",
    "\n",
    "    '''\n",
    "    return nltk.tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunks(sentences):\n",
    "  '''\n",
    "  This module splits the entire corpus into chunks. A chunk is a list of sentences from the corpus. This module add a sentence to a chunk if the tokenized sentence doesn't exceed the models' single sentence length. Else it adds the senence to next chunk.\n",
    "\n",
    "  Parameters:\n",
    "  sentences : list\n",
    "    A list of containing sentences.\n",
    "\n",
    "  Returns:\n",
    "  chunks : list\n",
    "    A list of containing the chunks of the document.\n",
    "  '''\n",
    "  length = 0\n",
    "  chunk = \"\"\n",
    "  chunks = []\n",
    "  count = -1\n",
    "  for sentence in sentences:\n",
    "    # print(\"Sent \", sentence)\n",
    "    count += 1\n",
    "    combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
    "\n",
    "    if combined_length  < tokenizer.max_len_single_sentence - 3 : # if it doesn't exceed\n",
    "      chunk += sentence + \" \" # add the sentence to the chunk\n",
    "      length = combined_length # update the length counter\n",
    "      # print(\"length\",length)\n",
    "      # if it is the last sentence\n",
    "      if count == len(sentences) - 1:\n",
    "        chunks.append(chunk.strip()) # save the chunk\n",
    "      # print(\"if\", chunks)\n",
    "    else:\n",
    "      chunks.append(chunk.strip()) # save the chunk\n",
    "      # print(\"else\", chunks)\n",
    "\n",
    "      # reset\n",
    "      length = 0\n",
    "      chunk = \"\"\n",
    "\n",
    "      # take care of the overflow sentence\n",
    "      chunk += sentence + \" \"\n",
    "      length = len(tokenizer.tokenize(sentence))\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_summary(chunk):\n",
    "    '''\n",
    "    This module generates the summary for a given document. Prior to passing the document to model, it appends 'summarize: ' prefix to document to let the model know that the task is summarization. It restricts the generated summary to 100 tokens.\n",
    "\n",
    "    Parameters:\n",
    "    chunk : str\n",
    "        A chunk is group of sentences such that the tokenized lenght of chunk doesn't exceed the token limit of model.\n",
    "\n",
    "    Returns:\n",
    "    summary : str\n",
    "        It is the summary which model generates for the given text chunk. \n",
    "    '''\n",
    "    input_ids = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\",max_length=tokenizer.model_max_length, truncation=True)\n",
    "    summary_ids = model.generate(input_ids,max_length=100)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(fileChunks):\n",
    "    '''\n",
    "    This function returns the complete summary for a chunked document or corpus of text.\n",
    "\n",
    "    Parameters:\n",
    "    fileChunks: list\n",
    "        A list of chunk where chunk is group of sentences in a corpus such that tokenized length of chunk doesn't exceed the model limit.\n",
    "\n",
    "    Returns:\n",
    "        Concatenated Summary as *str* for every chunk. \n",
    "    '''\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in fileChunks:\n",
    "        summ = generate_chunk_summary(chunk)\n",
    "        summaries.append(summ)\n",
    "\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ./generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_summary(generated_summary:str)->str:\n",
    "    '''\n",
    "    This function caps the generated summary to 1000 words. \n",
    "\n",
    "    Parameters:\n",
    "    generated_summary : str\n",
    "        System generated summary.\n",
    "\n",
    "    Returns:\n",
    "        Returns Summary as str after capping.\n",
    "    '''\n",
    "    sentences = nltk.sent_tokenize(generated_summary)\n",
    "    new_summary=[]\n",
    "    target_word=1250\n",
    "    words_added=0\n",
    "    for sentence in sentences:\n",
    "        words= nltk.word_tokenize(sentence)\n",
    "        if words_added + len(words) <= target_word:\n",
    "            words_added+=len(words)\n",
    "            new_summary.append(sentence)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \"\".join(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    '''\n",
    "    This function generates the summary for all the documents in *data-dir* . It generates the summary or the document and writes the summary in *generated_summary* folder. The convention for written file is *filename_sysSumm.txt*\n",
    "    '''\n",
    "    data_dir = \"./fns2020_dataset/validation/annual_reports/\"\n",
    "    for file in os.listdir(data_dir):\n",
    "        \n",
    "        fileContent = read_file(os.path.join(data_dir,file))\n",
    "        filePreprocessed = preprocessing(fileContent)\n",
    "        fileSentences = createSentences(filePreprocessed)\n",
    "        fileChunks = createChunks(fileSentences)\n",
    "        summary = generate_summary(fileChunks)\n",
    "        filename = file[:-4]\n",
    "\n",
    "        with open(f\"./generated_summary/{filename}_sysSumm.txt\",\"w+\") as f:\n",
    "            f.write(summary)\n",
    "        \n",
    "\n",
    "\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./generated_summary/\"\n",
    "target_dir= \"./capped_summary/\"\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    filename = os.path.basename(os.path.join(data_dir,file))\n",
    "    generated_summary = open(os.path.join(data_dir,file),\"r\").read().strip()\n",
    "\n",
    "    capped_summary = cap_summary(generated_summary)\n",
    "\n",
    "    with open(os.path.join(target_dir,filename),\"w\") as f:\n",
    "        f.write(capped_summary)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
