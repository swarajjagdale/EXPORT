{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd transformers && git reset --hard 143738214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/student/.local/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/student/anaconda3/envs/NLP/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /home/student/anaconda3/envs/NLP/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/student/anaconda3/envs/NLP/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/student/anaconda3/envs/NLP/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/student/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/student/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/student/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/anaconda3/envs/NLP/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/anaconda3/envs/NLP/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/student/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/student/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, oauthlib\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.0\n",
      "    Uninstalling urllib3-2.2.0:\n",
      "      Successfully uninstalled urllib3-2.2.0\n",
      "Successfully installed oauthlib-3.2.2 urllib3-1.26.18\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface\n",
      "  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: huggingface\n",
      "Successfully installed huggingface-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following yanked versions: 0.12.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tokenizers==4.3 (from versions: 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.1.0, 0.1.1, 0.2.0, 0.2.1, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.5.0, 0.5.1, 0.5.2, 0.6.0, 0.7.0rc1, 0.7.0rc2, 0.7.0rc3, 0.7.0rc4, 0.7.0rc5, 0.7.0rc6, 0.7.0rc7, 0.7.0, 0.8.0.dev0, 0.8.0.dev1, 0.8.0.dev2, 0.8.0rc1, 0.8.0rc2, 0.8.0rc3, 0.8.0rc4, 0.8.0, 0.8.1rc1, 0.8.1rc2, 0.8.1, 0.9.0.dev0, 0.9.0.dev1, 0.9.0.dev2, 0.9.0.dev3, 0.9.0.dev4, 0.9.0rc1, 0.9.0rc2, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.10.0rc1, 0.10.0, 0.10.1rc1, 0.10.1, 0.10.2, 0.10.3, 0.11.0, 0.11.1, 0.11.2, 0.11.3, 0.11.4, 0.11.5, 0.11.6, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3rc1, 0.13.3, 0.13.4rc1, 0.13.4rc2, 0.13.4rc3, 0.14.0rc1, 0.14.0, 0.14.1rc1, 0.14.1, 0.15.0rc1, 0.15.0, 0.15.1rc0, 0.15.1rc1, 0.15.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tokenizers==4.3\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install tokenizers[torch]==4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# Initialize our model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"orzhan/t5-long-extract\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"orzhan/t5-long-extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.37.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/student/anaconda3/envs/mypython310/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filenameWithPath:str)->str:\n",
    "  '''\n",
    "  This function reads the content of a file.\n",
    "\n",
    "  Parameters:\n",
    "  filenameWithPath : str\n",
    "    Absolute Path of the file which is to be read.\n",
    "\n",
    "  Returns:\n",
    "  text : str\n",
    "    The content of the read file. \n",
    "\n",
    "  '''\n",
    "  file = open(filenameWithPath,\"r\")\n",
    "  text = file.read().strip()\n",
    "  text = re.sub(\"\\n\",\" \",text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "  '''\n",
    "  The preprocessing function which parses raw text and manages to clean the raw text. This module removes websites/URLs, Email IDs, redundant spaces, extra periods, dashes and commas. It also replace some special unicode apostrope, qoutation symbols with ' and  \". This module also ads extra space after period, question mark and exclamation mark.\n",
    "\n",
    "  Parameters:\n",
    "    text : str\n",
    "      The raw text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "    text : str\n",
    "      The preprocessed text. \n",
    "  '''\n",
    "  # text = text.lower()\n",
    "  text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" \", text)\n",
    "  text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', \" \",text)\n",
    "  text = re.sub(\"\\u2019\", '\\'', text)\n",
    "  text = re.sub(\"\\u2018\", '\\'', text)\n",
    "  text = re.sub(\"\\u201C\", '\\\"', text)\n",
    "  text = re.sub(\"\\u201D\", '\\\"', text)\n",
    "  # text = re.sub(\"[$]\", \"dollar \", text)\n",
    "  # text = re.sub(\"[£]\", \"pound \", text)\n",
    "  # text = re.sub(\"[%]\", \" percent\", text)\n",
    "  # text = re.sub(r\"[^a-zA-Z0-9?!.,’-]\", ' ', text)\n",
    "  text = re.sub(r\"([?!])\", r\" \\1 \", text)\n",
    "  text = re.sub(r',+', ',', text)\n",
    "  text = re.sub(r'[-]+', ' ', text)\n",
    "  text = re.sub(r'([a-z])(?=[.,])', r'\\1 ', text)\n",
    "  text = re.sub(r'\\.{2,}', '.', text)\n",
    "  text = re.sub(r\"\\s+\", \" \", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSentences(text):\n",
    "    '''\n",
    "    This function uses the nltk library to sentence tokenize a given corpus or document.\n",
    "\n",
    "    Parameters:\n",
    "    text : \n",
    "        The corpus or document string to split into sentences.\n",
    "\n",
    "    Returns: \n",
    "        A sentence tokenized copy of *text* using NLTK's default tokenizer. A list of sentences.\n",
    "\n",
    "    '''\n",
    "    return nltk.tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunks(sentences):\n",
    "  '''\n",
    "  This module splits the entire corpus into chunks. A chunk is a list of sentences from the corpus. This module add a sentence to a chunk if the tokenized sentence doesn't exceed the models' single sentence length. Else it adds the senence to next chunk.\n",
    "\n",
    "  Parameters:\n",
    "  sentences : list\n",
    "    A list of containing sentences.\n",
    "\n",
    "  Returns:\n",
    "  chunks : list\n",
    "    A list of containing the chunks of the document.\n",
    "  '''\n",
    "  length = 0\n",
    "  chunk = \"\"\n",
    "  chunks = []\n",
    "  count = -1\n",
    "  for sentence in sentences:\n",
    "    # print(\"Sent \", sentence)\n",
    "    count += 1\n",
    "    combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
    "\n",
    "    if combined_length  < tokenizer.max_len_single_sentence - 3 : # if it doesn't exceed\n",
    "      chunk += sentence + \" \" # add the sentence to the chunk\n",
    "      length = combined_length # update the length counter\n",
    "      # print(\"length\",length)\n",
    "      # if it is the last sentence\n",
    "      if count == len(sentences) - 1:\n",
    "        chunks.append(chunk.strip()) # save the chunk\n",
    "      # print(\"if\", chunks)\n",
    "    else:\n",
    "      chunks.append(chunk.strip()) # save the chunk\n",
    "      # print(\"else\", chunks)\n",
    "\n",
    "      # reset\n",
    "      length = 0\n",
    "      chunk = \"\"\n",
    "\n",
    "      # take care of the overflow sentence\n",
    "      chunk += sentence + \" \"\n",
    "      length = len(tokenizer.tokenize(sentence))\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_summary(chunk):\n",
    "    '''\n",
    "    This module generates the summary for a given document. Prior to passing the document to model, it appends 'summarize: ' prefix to document to let the model know that the task is summarization. It restricts the generated summary to 100 tokens.\n",
    "\n",
    "    Parameters:\n",
    "    chunk : str\n",
    "        A chunk is group of sentences such that the tokenized lenght of chunk doesn't exceed the token limit of model.\n",
    "\n",
    "    Returns:\n",
    "    summary : str\n",
    "        It is the summary which model generates for the given text chunk. \n",
    "    '''\n",
    "    input_ids = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\",max_length=tokenizer.model_max_length, truncation=True)\n",
    "    summary_ids = model.generate(input_ids,max_length=100)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(fileChunks):\n",
    "    '''\n",
    "    This function returns the complete summary for a chunked document or corpus of text.\n",
    "\n",
    "    Parameters:\n",
    "    fileChunks: list\n",
    "        A list of chunk where chunk is group of sentences in a corpus such that tokenized length of chunk doesn't exceed the model limit.\n",
    "\n",
    "    Returns:\n",
    "        Concatenated Summary as *str* for every chunk. \n",
    "    '''\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in fileChunks:\n",
    "        summ = generate_chunk_summary(chunk)\n",
    "        summaries.append(summ)\n",
    "\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_summary(generated_summary:str)->str:\n",
    "    '''\n",
    "    This function caps the generated summary to 1000 words. \n",
    "\n",
    "    Parameters:\n",
    "    generated_summary : str\n",
    "        System generated summary.\n",
    "\n",
    "    Returns:\n",
    "        Returns Summary as str after capping.\n",
    "    '''\n",
    "    sentences = nltk.sent_tokenize(generated_summary)\n",
    "    new_summary=[]\n",
    "    target_word=1250\n",
    "    words_added=0\n",
    "    for sentence in sentences:\n",
    "        words= nltk.word_tokenize(sentence)\n",
    "        if words_added + len(words) <= target_word:\n",
    "            words_added+=len(words)\n",
    "            new_summary.append(sentence)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \"\".join(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    '''\n",
    "    This function generates the summary for all the documents in *data-dir* . It generates the summary or the document and writes the summary in *generated_summary* folder. The convention for written file is *filename_sysSumm.txt*\n",
    "    '''\n",
    "    data_dir = os.getenv('VAL_AR')\n",
    "    for file in os.listdir(data_dir):\n",
    "        \n",
    "        fileContent = read_file(os.path.join(data_dir,file))\n",
    "        filePreprocessed = preprocessing(fileContent)\n",
    "        fileSentences = createSentences(filePreprocessed)\n",
    "        fileChunks = createChunks(fileSentences)\n",
    "        summary = generate_summary(fileChunks)\n",
    "        filename = file[:-4]\n",
    "\n",
    "        with open(f\"./{filename}_sysSumm.txt\",\"w+\") as f:\n",
    "            f.write(summary)\n",
    "\n",
    "        break\n",
    "        \n",
    "\n",
    "\n",
    "summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
